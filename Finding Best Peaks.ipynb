{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d0050ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.signal import find_peaks\n",
    "from scipy import signal\n",
    "from scipy.optimize import curve_fit\n",
    "import cmath\n",
    "import control\n",
    "from control.matlab import *\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "from lmfit.models import LorentzianModel, QuadraticModel,LinearModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119300de",
   "metadata": {},
   "source": [
    "# Lorentzian Decomposition\n",
    "The current report shows how we extract the global lorentzian decomposition of the plate experiment. The goal is to get a first approximation of the main frequencies that excite the different modes, and subsequently model each spatial point of the plate with these known frequencies.\n",
    "\n",
    "To achieve this, we will fit the absolute value of the sum of the spectral descomposition of every point in the plate and extract the frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7cb2d2",
   "metadata": {},
   "source": [
    "### 1.0 Data Manipulation\n",
    "Extracting the data, transforming it to the frequency domain and getting the sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3838976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdf5storage\n",
    "data = hdf5storage.loadmat('data/final_data.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8d7d4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['XX', 'Xmatrix', 'YY', 'ZZ', 'f0', 'name2Save', 'sampleintervalS'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faa40964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1780"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1900-120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8959589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_sum = 0\n",
    "ffts = {}\n",
    "\n",
    "Fc_low = 13500\n",
    "Fc_high = 1000\n",
    "L = 1880\n",
    "Fs = 199999\n",
    "colors = ['blue','green','orange','red']\n",
    "\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "\n",
    "for i in range(118):\n",
    "    for j in range(118):\n",
    "        \n",
    "        #results[f'[{i},{j}]'] = {'params': [] , 'red_chi': [],'chi_sq': []}\n",
    "        v = data['ZZ'][:,i,j][120:2000]\n",
    "\n",
    "        sos_l = signal.bessel(5,Fc_low, 'low', fs=Fs,output='sos') # LPF filtering parameters\n",
    "        sos_h = signal.bessel(5,Fc_high, 'high', fs=Fs,output='sos') # HPF filtering parameters\n",
    "\n",
    "        x = signal.sosfilt(sos_l,v) # LPF\n",
    "        x = signal.sosfilt(sos_h,x) # HPF\n",
    "\n",
    "        freqs=np.fft.fftfreq(L,1/Fs)\n",
    "        f = freqs[freqs>400] #frequency vector (x-axis in frec domain)\n",
    "        fft= np.fft.fft(x)[freqs>400]\n",
    "        \n",
    "        ffts[f'{i},{j}'] = fft\n",
    "        fft_sum = fft_sum + fft\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e10c108e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "936"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ffts['74,33'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a6c1230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'FFT (Abs)')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "plt.plot(f,np.abs(fft_sum)/(117*10),'-o')\n",
    "plt.plot(f,np.abs(ffts['74,33']),'-o')\n",
    "index = [np.where(f>p)[0][0] for i,p in enumerate(p20)]\n",
    "plt.plot(f[index],np.abs(fft_sum[index])/(117*10),'o')\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.ylabel('FFT (Abs)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87997107",
   "metadata": {},
   "source": [
    "### 1.0 Looking at the maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "16cf675e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total points: 13924\n",
      "Points that passed the threshold: 2507\n",
      "percentage of points that passed the threshold: 18.004883654122377 %\n"
     ]
    }
   ],
   "source": [
    "max_ = [max(ffts[k]) for k in ffts]\n",
    "\n",
    "a = np.array(max_)[np.array(max_)>30000]\n",
    "print(f'Total points: {len(max_)}')\n",
    "print(f'Points that passed the threshold: {len(a)}')\n",
    "print(f'percentage of points that passed the threshold: {len(a)/len(max_) * 100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d816f5",
   "metadata": {},
   "source": [
    "### 2.0  Estimating the Peaks \n",
    "Here we apply some functions that give us a first estimation of the amount and places of the peaks. The documentation is in section 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8eeba97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2dc949daf10>]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peaks = find_peaks_(f,np.abs(fft_sum))\n",
    "peaks = find_peaks(np.abs(fft_sum))[0]\n",
    "plt.plot(f[new_peaks],np.abs(fft_sum[new_peaks]),'o')\n",
    "plt.plot(f[index],np.abs(fft_sum[index]),'.')\n",
    "plt.plot(f,np.abs(fft_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "57669aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = [5390,5500,6169,6450,6865,7089,7544,7970,8672,8882,9221,9670,10016,10426,10673,11013,11800,12490]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2231b46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_peaks = [np.where(f>p)[0][0] for p in p2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b36e31e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c947436c",
   "metadata": {},
   "source": [
    "### 3.0 Finding the best Fit\n",
    "Curvefitting the dataset with our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ef1b5168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy import exp, loadtxt, pi, sqrt\n",
    "\n",
    "from lmfit import Model\n",
    "\n",
    "datx = f\n",
    "#tryonh with different points\n",
    "coord = '74,33'\n",
    "daty = ffts[coord]\n",
    "daty = fft_sum\n",
    "\n",
    "    \n",
    "mod = Model(MyLorentzian2)\n",
    "pars = mod.make_params()\n",
    "for i,p in enumerate(new_peaks):\n",
    "    prefix = 'lz%d_' % (i+1)\n",
    "    pars.add(name = f'{prefix}center', value = datx[p],min = min(datx),max = 13000)\n",
    "    pars.add(name = f'{prefix}sigma',value =  5, min=0)\n",
    "        #pars[prefix + 'amplitude'].set(daty[cen],min = min(daty),max = max(daty))\n",
    "    pars.add(name = f'{prefix}amplitude_real', value = daty[p].real,min = min(daty.real),max = max(daty.real) + 1)\n",
    "    pars.add(name = f'{prefix}amplitude_imag', value = daty[p].imag,min = min(daty.imag),max = max(daty.imag) + 1)\n",
    "    \n",
    "\n",
    "\n",
    "result = mod.fit(np.abs(daty), pars, x=datx)\n",
    "params_ = list(result.values.values())\n",
    "#print(result.fit_report())\n",
    "%matplotlib qt\n",
    "plt.figure()\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.ylabel('FFT (Abs)')\n",
    "plt.title(f'Point: {coord}')\n",
    "plt.plot(datx, np.abs(daty), 'bo-', label='Original Data (abs of sum)')\n",
    "plt.plot(datx, result.best_fit, 'r-',label='Best fit')\n",
    "#plt.plot(datx, result.best_fit, 'r-', label='best fit')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "25f6bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_new = [result.values[k] for k in result.values if 'center' in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5c6562e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5412.808271713509, 5612.11038571605, 6324.503304154022, 6380.4417622063465, 6406.121505718675, 6861.694279189701, 7077.525028825343, 7586.608794535136, 8251.356047802785, 8557.913141831705, 8881.526339770082, 8981.335585820168, 9224.594123834742, 10274.944365581925, 10293.965950948092, 10346.69327582523, 12732.42370972809, 13000.0]\n"
     ]
    }
   ],
   "source": [
    "print(centers_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d327c44",
   "metadata": {},
   "source": [
    "### 4.0 Fixin Centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8dc7067b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63,25\n",
      "41,72\n",
      "23,54\n",
      "95,98\n",
      "46,51\n"
     ]
    }
   ],
   "source": [
    "X = [25,72,54,98,51]\n",
    "Y = [63,41,23,95,46]\n",
    "type_ = ['strong','strong','weak','weak','banana']\n",
    "\n",
    "results = []\n",
    "datx = f\n",
    "\n",
    "for i in range(5):\n",
    "    print(f'{Y[i]},{X[i]}')\n",
    "    daty = ffts[f'{Y[i]},{X[i]}']\n",
    "\n",
    "    mod = Model(MyLorentzian2)\n",
    "    pars = mod.make_params()\n",
    "    for i in range(18):\n",
    "        prefix = 'lz%d_' % (i+1)\n",
    "        \n",
    "        pars.add(name = f'{prefix}center', value = datx[new_peaks[i]],vary=False)\n",
    "        mod.set_param_hint(f'{prefix}center', vary=False)\n",
    "        pars.add(name = f'{prefix}sigma',value = 50, min=0)\n",
    "            #pars[prefix + 'amplitude'].set(daty[cen],min = min(daty),max = max(daty))\n",
    "\n",
    "        #index_cen = np.where(f>main_f[i])[0][0]\n",
    "        pars.add(name = f'{prefix}amplitude_real', value = daty[new_peaks[i]].real)\n",
    "        pars.add(name = f'{prefix}amplitude_imag', value = daty[new_peaks[i]].imag)\n",
    "\n",
    "\n",
    "\n",
    "    result = mod.fit(np.abs(daty), pars, x=datx)\n",
    "    \n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d0b3b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,res in enumerate(results):\n",
    "    \n",
    "    fit = res.best_fit\n",
    "    daty = ffts[f'{Y[i]},{X[i]}']\n",
    "    plt.figure()\n",
    "    plt.xlabel('Frequency [Hz]')\n",
    "    plt.title(f'Fit for {type_[i]} position')\n",
    "    \n",
    "    plt.plot(f,np.abs(daty),'bo-',label='Data')\n",
    "    plt.plot(f,fit,'r-',label='Best Fit')\n",
    "    for i in range(1,19):\n",
    "\n",
    "        ps = [result.values[k] for k in result.values.keys() if f'lz{i}_' in k ]\n",
    "        #plt.plot(f,np.abs(Lorentzian(f,*ps)),ls='dashed')\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee53c63e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8e984e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6943e671",
   "metadata": {},
   "source": [
    "### 4.0 Functions \n",
    "These are the functions built to:\n",
    "\n",
    "~ Clean the data: Discrete Fourirer Transform the data, Bandpass frequencies\n",
    "\n",
    "~ Find Optimal amount of peaks: gets an initial guess of the peaks and discards the \"noisy\" ones, convolves the data with a gaussian function to smoothen it, and searches for the rest of the peaks.\n",
    "\n",
    "~ Get the best fit of our Lorentzian Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a4fb7c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_clean_data(ydata,Fs,Fc_low,Fc_high):\n",
    "    \"\"\"fft the data,bandpasses the frequencies in the chosen range\"\"\"\n",
    "    # low pass filter\n",
    "    L = len(ydata) \n",
    "\n",
    "    sos_l = signal.bessel(15,Fc_low, 'low', fs=Fs,output='sos') # LPF filtering parameters\n",
    "    sos_h = signal.bessel(15,Fc_high, 'high', fs=Fs,output='sos') # HPF filtering parameters\n",
    "\n",
    "    X = signal.sosfilt(sos_l,v) # LPF\n",
    "    X = signal.sosfilt(sos_h,X) # HPF\n",
    "\n",
    "    freqs=np.fft.fftfreq(L,1/Fs) #frequency vector (x-axis in frec domain)\n",
    "    fft= np.fft.fft(X)# displacement band passes transform \n",
    "    #fftPhase = [cmath.phase(z) for z in np.fft.fft(v)]\n",
    "\n",
    "    return freqs,fft\n",
    "\n",
    "def norm(x,y):\n",
    "    \"\"\"euclidean distance\"\"\"\n",
    "    return np.sqrt(x**2 + y**2)\n",
    "\n",
    "def consecutive_peaks(peak_index):\n",
    "    \"\"\"returns in sublists the consecutive indexes\"\"\"\n",
    "    sub_list = np.split(peak_index, np.where(np.diff(peak_index) > 1)[0] + 1)\n",
    "    return sub_list\n",
    "   \n",
    "    \n",
    "def find_near_peaks(x,y,peaks):\n",
    "    \"\"\" gets the indexes of the peaks that are 'near' each other. \n",
    "    'near': the distance beaing a higher order than the average . \n",
    "    Can be modified depending on the noise of the data (as it creates micro-peaks)\n",
    "    \"\"\"\n",
    "    near_peaks = []\n",
    "    peak_dist = norm(np.diff(x[peaks]),np.diff(y[peaks]))  #distance between each consecutive peak\n",
    "    avg_peak_dist = np.mean(peak_dist)*10 # 1 higher order than mean of distance between peaks \n",
    "    for i,peak in enumerate(peaks):\n",
    "        if i == 0:\n",
    "            dist_next = norm(np.diff([x[peak],x[peaks[i+1]]]),np.diff([y[peak],y[peaks[i+1]]]))\n",
    "            if dist_next < avg_peak_dist:\n",
    "                near_peaks.append(peak)\n",
    "        elif i == len(peaks)-1:\n",
    "            dist_prev = norm(np.diff([x[peak],x[peaks[i-1]]]),np.diff([y[peak],y[peaks[i-1]]]))\n",
    "            if dist_prev < avg_peak_dist:\n",
    "                near_peaks.append(peak)\n",
    "        else:\n",
    "            dist_next = norm(np.diff([x[peak],x[peaks[i+1]]]),np.diff([y[peak],y[peaks[i+1]]]))\n",
    "            dist_prev = norm(np.diff([x[peak],x[peaks[i-1]]]),np.diff([y[peak],y[peaks[i-1]]]))\n",
    "            if dist_next < avg_peak_dist or dist_prev < avg_peak_dist:\n",
    "                near_peaks.append(peak)\n",
    "    return near_peaks\n",
    "\n",
    "\n",
    "def find_peaks_(x,y):\n",
    "    \"\"\"finds the peak with scipy function\n",
    "    Saves the peaks of the original data that are an order higher than the noise.\n",
    "    For the noise peaks, saves only the peaks of a convolution between a Gaussian distrib. with 10 std and the \n",
    "    original function.\n",
    "    \"\"\"\n",
    "    peaks, _ = find_peaks(y)\n",
    "    \n",
    "    near_peaks = find_near_peaks(x,y,peaks)\n",
    "    \n",
    "    peaks_ = np.setdiff1d(peaks,near_peaks) # not near peaks\n",
    "\n",
    "    gaussian = signal.windows.gaussian(len(y), std=10)\n",
    "    \n",
    "    convolution = signal.fftconvolve(y, gaussian, mode='same')\n",
    "\n",
    "    peak_list, _ = find_peaks(convolution)\n",
    "\n",
    "    peaks_ = np.append(peaks_,peak_list)\n",
    "    \n",
    "    peaks_ = [p for p in peaks_ if x[p] <= 20000] #deletes the high frequency peaks (higher than 20kHz)\n",
    "    return peaks_\n",
    "\n",
    "\n",
    "\n",
    "def add_peak(prefix, center, amplitude, sigma):\n",
    "    \"\"\"adds an additional lorentzian to the model. none parameters fixed\"\"\"\n",
    "    peak = LorentzianModel(prefix=prefix)\n",
    "    pars = Parameters()\n",
    "    pars.add(name=prefix + 'amplitude', value=amplitude)\n",
    "    pars.add(name=prefix + 'center',value = center)\n",
    "    pars.add(name=prefix + 'sigma', value = sigma, min=0)\n",
    "    pars.add(name=prefix + 'fwhm', expr=f'2.0000000*{prefix}sigma')\n",
    "    pars.add(name=prefix + 'height', expr=f'0.3183099*{prefix}amplitude/max(1e-15, {prefix}sigma)')\n",
    "    return peak, pars\n",
    "\n",
    "\n",
    "def get_cen(out):\n",
    "    ln = set([k.split('_')[0] for k in out])\n",
    "    cen = []\n",
    "    amp = []\n",
    "    for l in ln:\n",
    "        if np.abs(out[f'{l}_amplitude']) > 1:\n",
    "            cen.append(out[f'{l}_center'])\n",
    "            amp.append(out[f'{l}_amplitude'])\n",
    "    return cen\n",
    "\n",
    "def MyLorentzian2(x,lz1_center,lz1_sigma,lz1_amplitude_real,lz1_amplitude_imag,lz2_center,lz2_sigma,lz2_amplitude_real,lz2_amplitude_imag,lz3_center,lz3_sigma,lz3_amplitude_real,lz3_amplitude_imag,lz4_center,lz4_sigma,lz4_amplitude_real,lz4_amplitude_imag,lz5_center,lz5_sigma,lz5_amplitude_real,lz5_amplitude_imag,lz6_center,lz6_sigma,lz6_amplitude_real,lz6_amplitude_imag,lz7_center,lz7_sigma,lz7_amplitude_real,lz7_amplitude_imag,lz8_center,lz8_sigma,lz8_amplitude_real,lz8_amplitude_imag,lz9_center,lz9_sigma,lz9_amplitude_real,lz9_amplitude_imag,lz10_center,lz10_sigma,lz10_amplitude_real,lz10_amplitude_imag,lz11_center,lz11_sigma,lz11_amplitude_real,lz11_amplitude_imag,lz12_center,lz12_sigma,lz12_amplitude_real,lz12_amplitude_imag,lz13_center,lz13_sigma,lz13_amplitude_real,lz13_amplitude_imag,lz14_center,lz14_sigma,lz14_amplitude_real,lz14_amplitude_imag,lz15_center,lz15_sigma,lz15_amplitude_real,lz15_amplitude_imag,lz16_center,lz16_sigma,lz16_amplitude_real,lz16_amplitude_imag,lz17_center,lz17_sigma,lz17_amplitude_real,lz17_amplitude_imag,lz18_center,lz18_sigma,lz18_amplitude_real,lz18_amplitude_imag):\n",
    "    \"\"\"returns the absolute values of the sum of 13 lorentzian function\"\"\"\n",
    "    \n",
    "    lz1_amplitude =  lz1_amplitude_real + 1j*lz1_amplitude_imag\n",
    "    lz2_amplitude =  lz2_amplitude_real + 1j*lz2_amplitude_imag\n",
    "    lz3_amplitude =  lz3_amplitude_real + 1j*lz3_amplitude_imag\n",
    "    lz4_amplitude =  lz4_amplitude_real + 1j*lz4_amplitude_imag\n",
    "    lz5_amplitude =  lz5_amplitude_real + 1j*lz5_amplitude_imag\n",
    "    lz6_amplitude =  lz6_amplitude_real + 1j*lz6_amplitude_imag\n",
    "    lz7_amplitude =  lz7_amplitude_real + 1j*lz7_amplitude_imag\n",
    "    lz8_amplitude =  lz8_amplitude_real + 1j*lz8_amplitude_imag\n",
    "    lz9_amplitude =  lz9_amplitude_real + 1j*lz9_amplitude_imag\n",
    "    lz10_amplitude =  lz10_amplitude_real + 1j*lz10_amplitude_imag\n",
    "    lz11_amplitude =  lz11_amplitude_real + 1j*lz11_amplitude_imag\n",
    "    lz12_amplitude =  lz12_amplitude_real + 1j*lz12_amplitude_imag\n",
    "    lz13_amplitude =  lz13_amplitude_real + 1j*lz13_amplitude_imag\n",
    "    lz14_amplitude =  lz14_amplitude_real + 1j*lz14_amplitude_imag\n",
    "    lz15_amplitude =  lz15_amplitude_real + 1j*lz15_amplitude_imag\n",
    "    lz16_amplitude =  lz16_amplitude_real + 1j*lz16_amplitude_imag\n",
    "    lz17_amplitude =  lz17_amplitude_real + 1j*lz17_amplitude_imag\n",
    "    lz18_amplitude =  lz18_amplitude_real + 1j*lz18_amplitude_imag\n",
    "\n",
    "\n",
    "    l1 = (lz1_amplitude * lz1_sigma**2) / ( lz1_sigma**2 + ( x - lz1_center )**2) \n",
    "    l2 = (lz2_amplitude * lz2_sigma**2) / ( lz2_sigma**2 + ( x - lz2_center )**2) \n",
    "    l3 = (lz3_amplitude * lz3_sigma**2) / ( lz3_sigma**2 + ( x - lz3_center )**2)\n",
    "    l4 = (lz4_amplitude * lz4_sigma**2) / ( lz4_sigma**2 + ( x - lz4_center )**2) \n",
    "    l5 = (lz5_amplitude * lz5_sigma**2) / ( lz5_sigma**2 + ( x - lz5_center )**2)\n",
    "    l6 = (lz6_amplitude * lz6_sigma**2) / ( lz6_sigma**2 + ( x - lz6_center )**2) \n",
    "    l7 = (lz7_amplitude * lz7_sigma**2) / ( lz7_sigma**2 + ( x - lz7_center )**2) \n",
    "    l8 = (lz8_amplitude * lz8_sigma**2) / ( lz8_sigma**2 + ( x - lz8_center )**2)\n",
    "    l9 = (lz9_amplitude * lz9_sigma**2) / ( lz9_sigma**2 + ( x - lz9_center )**2) \n",
    "    l10 = (lz10_amplitude * lz10_sigma**2) / ( lz10_sigma**2 + ( x - lz10_center )**2) \n",
    "    l11 = (lz11_amplitude * lz11_sigma**2) / ( lz11_sigma**2 + ( x - lz11_center )**2) \n",
    "    l12 = (lz12_amplitude * lz12_sigma**2) / ( lz12_sigma**2 + ( x - lz12_center )**2) \n",
    "    l13 = (lz13_amplitude * lz13_sigma**2) / ( lz13_sigma**2 + ( x - lz13_center )**2)\n",
    "    l14 = (lz14_amplitude * lz14_sigma**2) / ( lz14_sigma**2 + ( x - lz14_center )**2) \n",
    "    l15 = (lz15_amplitude * lz15_sigma**2) / ( lz15_sigma**2 + ( x - lz15_center )**2)\n",
    "    l16 = (lz16_amplitude * lz16_sigma**2) / ( lz16_sigma**2 + ( x - lz16_center )**2) \n",
    "    l17 = (lz17_amplitude * lz17_sigma**2) / ( lz17_sigma**2 + ( x - lz17_center )**2) \n",
    "    l18 = (lz18_amplitude * lz18_sigma**2) / ( lz18_sigma**2 + ( x - lz18_center )**2)\n",
    "    \n",
    "    return np.abs(l1+l2+l3+l4+l5+l6+l7+l8+l9+l10+l11+l12+l13+l14+l15+l16+l17+l18)\n",
    "\n",
    "def Lorentzian(x,lz1_center,lz1_sigma,lz1_amplitude_real,lz1_amplitude_imag):\n",
    "    \"\"\"returns the absolute values of the sum of 13 lorentzian function\"\"\"\n",
    "    \n",
    "    lz1_amplitude =  lz1_amplitude_real + 1j*lz1_amplitude_imag\n",
    "\n",
    "    l1 = (lz1_amplitude * lz1_sigma**2) / ( lz1_sigma**2 + ( x - lz1_center )**2) \n",
    "\n",
    "    \n",
    "    return l1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52b6072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs = 199999 #sample freq \n",
    "Fc_Low = 18000 # cutting frequency low pass filter\n",
    "Fc_High = 4000 # cutting frequency high pass filter\n",
    "fft_sum = 0\n",
    "for i in range(31):\n",
    "    for j in range(31):\n",
    "        v = data[:,i,j]\n",
    "        f, fft = fft_clean_data(v,Fs,Fc_Low,Fc_High)\n",
    "        fft_sum = fft_sum + fft\n",
    "L = len(v)\n",
    "f = f[10:L//2] \n",
    "fft = fft[10:L//2]      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abecfa01",
   "metadata": {},
   "source": [
    "### Functions (more)\n",
    "For future use: fixing the known frequencies to each point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6b427029",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# based on https://lmfit.github.io/lmfit-py/builtin_models.html#lorentzianmodel\n",
    "from lmfit import Model, Parameters\n",
    "\n",
    "\n",
    "\n",
    "def lorentzians_optimization(datx,daty,main_frequencies,main_amplitudes):\n",
    "    %matplotlib qt\n",
    "    mod_i = []\n",
    "    for i, frec in enumerate(main_frequencies):\n",
    "        #frec_index = datx.index(frec)\n",
    "        if i ==0:\n",
    "            peak, pars = add_peak_('lz%d_' % (i+1), frec ,main_amplitudes[i],5)\n",
    "            print(pars)\n",
    "            params = peak.make_params(pars)\n",
    "        else:\n",
    "            peak, pars = add_peak_('lz%d_' % (i+1), frec ,main_amplitudes[i],5)\n",
    "            params.update(pars)\n",
    "        mod_i.append(peak)\n",
    "\n",
    "    mod = mod_i[0]\n",
    "    mod_i.pop(0)\n",
    "    for i in mod_i:\n",
    "        mod = mod + i\n",
    "\n",
    "    init = mod.eval(params, x=datx)\n",
    "    out = mod.fit(daty, params, x=datx)\n",
    "\n",
    "    print(out.fit_report(min_correl=0.5))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12.8, 4.8))\n",
    "    axes[0].plot(datx, daty, 'b',label='Original Data')\n",
    "    axes[0].plot(datx, init, 'k--', label='Initial fit')\n",
    "    axes[0].plot(datx, out.best_fit, 'r-', label='Best fit')\n",
    "    axes[0].legend(loc='best')\n",
    "\n",
    "    axes = axes.flatten()\n",
    "    comps = out.eval_components(x=datx)\n",
    "    axes[1].plot(datx, daty, 'b')\n",
    "    for i, cen in enumerate(main_frequencies):\n",
    "        axes[1].plot(datx, comps['lz%d_' % (i+1)], '--', label=f'Lorentzian {i}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def lorentzian( x, x0, a, gam ,x1,a1,gam1):\n",
    "    l = (a * gam**2 / ( gam**2 + ( x - x0 )**2) )+ (a1 * gam1**2 / ( gam1**2 + ( x - x1 )**2))\n",
    "    l_noisy = l+np.random.normal(0,0.01,x.shape)\n",
    "    return l_noisy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
