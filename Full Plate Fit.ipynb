{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b3f346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.signal import find_peaks\n",
    "from scipy import signal\n",
    "from scipy.optimize import curve_fit\n",
    "import cmath\n",
    "import control\n",
    "from control.matlab import *\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "import h5py\n",
    "from lmfit.models import LorentzianModel, QuadraticModel,LinearModel\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import exp, loadtxt, pi, sqrt\n",
    "from lmfit import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e663337c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['XX', 'YY', 'f0', 'name2Save', 'sampleintervalS'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hdf5storage\n",
    "folder = 'data'\n",
    "file_name = 'final_data.mat'\n",
    "data = hdf5storage.loadmat(f'{folder}/{file_name}')\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa066b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_centers = [5412.808271713509, 5612.11038571605, 6324.503304154022, 6380.4417622063465, 6406.121505718675, 6861.694279189701, 7077.525028825343, 7586.608794535136, 8251.356047802785, 8557.913141831705, 8881.526339770082, 8981.335585820168, 9224.594123834742, 10274.944365581925, 10293.965950948092, 10346.69327582523, 12732.42370972809, 13000.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f9d812",
   "metadata": {},
   "source": [
    "### RUNNING ALL THE POINTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f00ad95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0,0]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'ZZ'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8eff1531a3aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'[{i},{j}]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf'[{i},{j}]'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'red_chi'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'chi_sq'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ZZ'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m120\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0msos_l\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbessel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mFc_low\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'low'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sos'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# LPF filtering parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ZZ'"
     ]
    }
   ],
   "source": [
    "Fc_low = 13500\n",
    "Fc_high = 1000\n",
    "L = 1880\n",
    "Fs = 199999\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "results = {}\n",
    "\n",
    "for i in range(2): #goes through columns\n",
    "    if  i % 4 == 0 :\n",
    "        cut_file = i\n",
    "        results2 = results.copy()\n",
    "        results = {}\n",
    "    for j in range(118): #goes through rows\n",
    "        \n",
    "        if f'[{i},{j}]' not in results2.keys():\n",
    "            print(f'[{i},{j}]')\n",
    "            results[f'[{i},{j}]'] = {'params': [] , 'red_chi': [],'chi_sq': []}\n",
    "            v = data['ZZ'][:,i,j][120:2000]\n",
    "\n",
    "            sos_l = signal.bessel(5,Fc_low, 'low', fs=Fs,output='sos') # LPF filtering parameters\n",
    "            sos_h = signal.bessel(5,Fc_high, 'high', fs=Fs,output='sos') # HPF filtering parameters\n",
    "\n",
    "            x = signal.sosfilt(sos_l,v) # LPF\n",
    "            x = signal.sosfilt(sos_h,x) # HPF\n",
    "\n",
    "            freqs=np.fft.fftfreq(L,1/Fs)\n",
    "            f = freqs[freqs>100] #frequency vector (x-axis in frec domain)\n",
    "            fft= np.fft.fft(x)[freqs>100]\n",
    "            \n",
    "            if np.abs(max(fft)) >30000: #threshold for the amplitude (now passes abput 18% of all the points)\n",
    "                print('Nuevo:',i,j,np.abs(max(fft)))\n",
    "\n",
    "                mod = Model(MyLorentzian2) #here I call the name function of the model I want to create\n",
    "                pars = mod.make_params()\n",
    "                for n,p in enumerate(new_centers):\n",
    "                    index_p = np.where(f>p)[0][0]\n",
    "                    prefix = 'lz%d_' % (n+1)\n",
    "                    pars.add(name = f'{prefix}center', value = f[index_p],vary=False)\n",
    "                    mod.set_param_hint(f'{prefix}center', vary=False) #this tell the model to fix the parameter\n",
    "                    pars.add(name = f'{prefix}sigma',value =  5, min=0)\n",
    "                    pars.add(name = f'{prefix}amplitude_real', value = fft[index_p].real)\n",
    "                    pars.add(name = f'{prefix}amplitude_imag', value = fft[index_p].imag)\n",
    "\n",
    "                t1 = datetime.datetime.now() #just estimating the time it takes to the program to fit\n",
    "                result = mod.fit(np.abs(fft), pars, x=f,nan_policy='omit') #THIS IS THE FIT\n",
    "                t2 = datetime.datetime.now()\n",
    "                params_ = list(result.values.values())\n",
    "                results[f'[{i},{j}]']['params'] = list(params_)\n",
    "                results[f'[{i},{j}]']['red_chi']= result.redchi\n",
    "                results[f'[{i},{j}]']['chi_sq'] = result.chisqr\n",
    "\n",
    "                \n",
    "                print(f'Time: {t2-t1}')\n",
    "                with open(f'new_results2/{cut_file + 1}.txt', 'w') as file:\n",
    "                    file.truncate(0)\n",
    "                    file.write(json.dumps(results)) # use `json.loads` to do the reverse\n",
    "\n",
    "        #print(result.fit_report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b1b91c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lmfit.Model: Model(MyLorentzian2)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bea5d3",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a62f023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Lorentzian(x,lz1_center,lz1_sigma,lz1_amplitude_real,lz1_amplitude_imag):\n",
    "    \"\"\"returns the absolute values of the sum of 13 lorentzian function\"\"\"\n",
    "    \n",
    "    lz1_amplitude =  lz1_amplitude_real + 1j*lz1_amplitude_imag\n",
    "\n",
    "    l1 = (lz1_amplitude * lz1_sigma**2) / ( lz1_sigma**2 + ( x - lz1_center )**2) \n",
    "\n",
    "    \n",
    "    return l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75c6f8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyLorentzian2(x,lz1_center,lz1_sigma,lz1_amplitude_real,lz1_amplitude_imag,lz2_center,lz2_sigma,lz2_amplitude_real,lz2_amplitude_imag,lz3_center,lz3_sigma,lz3_amplitude_real,lz3_amplitude_imag,lz4_center,lz4_sigma,lz4_amplitude_real,lz4_amplitude_imag,lz5_center,lz5_sigma,lz5_amplitude_real,lz5_amplitude_imag,lz6_center,lz6_sigma,lz6_amplitude_real,lz6_amplitude_imag,lz7_center,lz7_sigma,lz7_amplitude_real,lz7_amplitude_imag,lz8_center,lz8_sigma,lz8_amplitude_real,lz8_amplitude_imag,lz9_center,lz9_sigma,lz9_amplitude_real,lz9_amplitude_imag,lz10_center,lz10_sigma,lz10_amplitude_real,lz10_amplitude_imag,lz11_center,lz11_sigma,lz11_amplitude_real,lz11_amplitude_imag,lz12_center,lz12_sigma,lz12_amplitude_real,lz12_amplitude_imag,lz13_center,lz13_sigma,lz13_amplitude_real,lz13_amplitude_imag,lz14_center,lz14_sigma,lz14_amplitude_real,lz14_amplitude_imag,lz15_center,lz15_sigma,lz15_amplitude_real,lz15_amplitude_imag,lz16_center,lz16_sigma,lz16_amplitude_real,lz16_amplitude_imag,lz17_center,lz17_sigma,lz17_amplitude_real,lz17_amplitude_imag,lz18_center,lz18_sigma,lz18_amplitude_real,lz18_amplitude_imag):\n",
    "    \"\"\"returns the absolute values of the sum of 13 lorentzian function\"\"\"\n",
    "    \n",
    "    lz1_amplitude =  lz1_amplitude_real + 1j*lz1_amplitude_imag\n",
    "    lz2_amplitude =  lz2_amplitude_real + 1j*lz2_amplitude_imag\n",
    "    lz3_amplitude =  lz3_amplitude_real + 1j*lz3_amplitude_imag\n",
    "    lz4_amplitude =  lz4_amplitude_real + 1j*lz4_amplitude_imag\n",
    "    lz5_amplitude =  lz5_amplitude_real + 1j*lz5_amplitude_imag\n",
    "    lz6_amplitude =  lz6_amplitude_real + 1j*lz6_amplitude_imag\n",
    "    lz7_amplitude =  lz7_amplitude_real + 1j*lz7_amplitude_imag\n",
    "    lz8_amplitude =  lz8_amplitude_real + 1j*lz8_amplitude_imag\n",
    "    lz9_amplitude =  lz9_amplitude_real + 1j*lz9_amplitude_imag\n",
    "    lz10_amplitude =  lz10_amplitude_real + 1j*lz10_amplitude_imag\n",
    "    lz11_amplitude =  lz11_amplitude_real + 1j*lz11_amplitude_imag\n",
    "    lz12_amplitude =  lz12_amplitude_real + 1j*lz12_amplitude_imag\n",
    "    lz13_amplitude =  lz13_amplitude_real + 1j*lz13_amplitude_imag\n",
    "    lz14_amplitude =  lz14_amplitude_real + 1j*lz14_amplitude_imag\n",
    "    lz15_amplitude =  lz15_amplitude_real + 1j*lz15_amplitude_imag\n",
    "    lz16_amplitude =  lz16_amplitude_real + 1j*lz16_amplitude_imag\n",
    "    lz17_amplitude =  lz17_amplitude_real + 1j*lz17_amplitude_imag\n",
    "    lz18_amplitude =  lz18_amplitude_real + 1j*lz18_amplitude_imag\n",
    "\n",
    "\n",
    "    l1 = (lz1_amplitude * lz1_sigma**2) / ( lz1_sigma**2 + ( x - lz1_center )**2) \n",
    "    l2 = (lz2_amplitude * lz2_sigma**2) / ( lz2_sigma**2 + ( x - lz2_center )**2) \n",
    "    l3 = (lz3_amplitude * lz3_sigma**2) / ( lz3_sigma**2 + ( x - lz3_center )**2)\n",
    "    l4 = (lz4_amplitude * lz4_sigma**2) / ( lz4_sigma**2 + ( x - lz4_center )**2) \n",
    "    l5 = (lz5_amplitude * lz5_sigma**2) / ( lz5_sigma**2 + ( x - lz5_center )**2)\n",
    "    l6 = (lz6_amplitude * lz6_sigma**2) / ( lz6_sigma**2 + ( x - lz6_center )**2) \n",
    "    l7 = (lz7_amplitude * lz7_sigma**2) / ( lz7_sigma**2 + ( x - lz7_center )**2) \n",
    "    l8 = (lz8_amplitude * lz8_sigma**2) / ( lz8_sigma**2 + ( x - lz8_center )**2)\n",
    "    l9 = (lz9_amplitude * lz9_sigma**2) / ( lz9_sigma**2 + ( x - lz9_center )**2) \n",
    "    l10 = (lz10_amplitude * lz10_sigma**2) / ( lz10_sigma**2 + ( x - lz10_center )**2) \n",
    "    l11 = (lz11_amplitude * lz11_sigma**2) / ( lz11_sigma**2 + ( x - lz11_center )**2) \n",
    "    l12 = (lz12_amplitude * lz12_sigma**2) / ( lz12_sigma**2 + ( x - lz12_center )**2) \n",
    "    l13 = (lz13_amplitude * lz13_sigma**2) / ( lz13_sigma**2 + ( x - lz13_center )**2)\n",
    "    l14 = (lz14_amplitude * lz14_sigma**2) / ( lz14_sigma**2 + ( x - lz14_center )**2) \n",
    "    l15 = (lz15_amplitude * lz15_sigma**2) / ( lz15_sigma**2 + ( x - lz15_center )**2)\n",
    "    l16 = (lz16_amplitude * lz16_sigma**2) / ( lz16_sigma**2 + ( x - lz16_center )**2) \n",
    "    l17 = (lz17_amplitude * lz17_sigma**2) / ( lz17_sigma**2 + ( x - lz17_center )**2) \n",
    "    l18 = (lz18_amplitude * lz18_sigma**2) / ( lz18_sigma**2 + ( x - lz18_center )**2)\n",
    "    \n",
    "    return np.abs(l1+l2+l3+l4+l5+l6+l7+l8+l9+l10+l11+l12+l13+l14+l15+l16+l17+l18)\n",
    "\n",
    "\n",
    "def MyLorentzian10(x,lz1_center,lz1_sigma,lz1_amplitude_real,lz1_amplitude_imag,lz2_center,lz2_sigma,lz2_amplitude_real,lz2_amplitude_imag,lz3_center,lz3_sigma,lz3_amplitude_real,lz3_amplitude_imag,lz4_center,lz4_sigma,lz4_amplitude_real,lz4_amplitude_imag,lz5_center,lz5_sigma,lz5_amplitude_real,lz5_amplitude_imag,lz6_center,lz6_sigma,lz6_amplitude_real,lz6_amplitude_imag,lz7_center,lz7_sigma,lz7_amplitude_real,lz7_amplitude_imag,lz8_center,lz8_sigma,lz8_amplitude_real,lz8_amplitude_imag,lz9_center,lz9_sigma,lz9_amplitude_real,lz9_amplitude_imag,lz10_center,lz10_sigma,lz10_amplitude_real,lz10_amplitude_imag):\n",
    "    \"\"\"returns the absolute values of the sum of 13 lorentzian function\"\"\"\n",
    "    \n",
    "    lz1_amplitude =  lz1_amplitude_real + 1j*lz1_amplitude_imag\n",
    "    lz2_amplitude =  lz2_amplitude_real + 1j*lz2_amplitude_imag\n",
    "    lz3_amplitude =  lz3_amplitude_real + 1j*lz3_amplitude_imag\n",
    "    lz4_amplitude =  lz4_amplitude_real + 1j*lz4_amplitude_imag\n",
    "    lz5_amplitude =  lz5_amplitude_real + 1j*lz5_amplitude_imag\n",
    "    lz6_amplitude =  lz6_amplitude_real + 1j*lz6_amplitude_imag\n",
    "    lz7_amplitude =  lz7_amplitude_real + 1j*lz7_amplitude_imag\n",
    "    lz8_amplitude =  lz8_amplitude_real + 1j*lz8_amplitude_imag\n",
    "    lz9_amplitude =  lz9_amplitude_real + 1j*lz9_amplitude_imag\n",
    "    lz10_amplitude =  lz10_amplitude_real + 1j*lz10_amplitude_imag\n",
    "\n",
    "\n",
    "    l1 = (lz1_amplitude * lz1_sigma**2) / ( lz1_sigma**2 + ( x - lz1_center )**2) \n",
    "    l2 = (lz2_amplitude * lz2_sigma**2) / ( lz2_sigma**2 + ( x - lz2_center )**2) \n",
    "    l3 = (lz3_amplitude * lz3_sigma**2) / ( lz3_sigma**2 + ( x - lz3_center )**2)\n",
    "    l4 = (lz4_amplitude * lz4_sigma**2) / ( lz4_sigma**2 + ( x - lz4_center )**2) \n",
    "    l5 = (lz5_amplitude * lz5_sigma**2) / ( lz5_sigma**2 + ( x - lz5_center )**2)\n",
    "    l6 = (lz6_amplitude * lz6_sigma**2) / ( lz6_sigma**2 + ( x - lz6_center )**2) \n",
    "    l7 = (lz7_amplitude * lz7_sigma**2) / ( lz7_sigma**2 + ( x - lz7_center )**2) \n",
    "    l8 = (lz8_amplitude * lz8_sigma**2) / ( lz8_sigma**2 + ( x - lz8_center )**2)\n",
    "    l9 = (lz9_amplitude * lz9_sigma**2) / ( lz9_sigma**2 + ( x - lz9_center )**2) \n",
    "    l10 = (lz10_amplitude * lz10_sigma**2) / ( lz10_sigma**2 + ( x - lz10_center )**2) \n",
    "\n",
    "\n",
    "    \n",
    "    return np.abs(l1+l2+l3+l4+l5+l6+l7+l8+l9+l10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5bdc425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x,y):\n",
    "    \"\"\"euclidean distance\"\"\"\n",
    "    return np.sqrt(x**2 + y**2)\n",
    "\n",
    "def consecutive_peaks(peak_index):\n",
    "    \"\"\"returns in sublists the consecutive indexes\"\"\"\n",
    "    sub_list = np.split(peak_index, np.where(np.diff(peak_index) > 1)[0] + 1)\n",
    "    return sub_list\n",
    "   \n",
    "    \n",
    "def find_near_peaks(x,y,peaks):\n",
    "    \"\"\" gets the indexes of the peaks that are 'near' each other. \n",
    "    'near': the distance beaing a higher order than the average . \n",
    "    Can be modified depending on the noise of the data (as it creates micro-peaks)\n",
    "    \"\"\"\n",
    "    near_peaks = []\n",
    "    peak_dist = norm(np.diff(x[peaks]),np.diff(y[peaks]))  #distance between each consecutive peak\n",
    "    avg_peak_dist = np.mean(peak_dist)*10 # 1 higher order than mean of distance between peaks \n",
    "    for i,peak in enumerate(peaks):\n",
    "        if i == 0:\n",
    "            dist_next = norm(np.diff([x[peak],x[peaks[i+1]]]),np.diff([y[peak],y[peaks[i+1]]]))\n",
    "            if dist_next < avg_peak_dist:\n",
    "                near_peaks.append(peak)\n",
    "        elif i == len(peaks)-1:\n",
    "            dist_prev = norm(np.diff([x[peak],x[peaks[i-1]]]),np.diff([y[peak],y[peaks[i-1]]]))\n",
    "            if dist_prev < avg_peak_dist:\n",
    "                near_peaks.append(peak)\n",
    "        else:\n",
    "            dist_next = norm(np.diff([x[peak],x[peaks[i+1]]]),np.diff([y[peak],y[peaks[i+1]]]))\n",
    "            dist_prev = norm(np.diff([x[peak],x[peaks[i-1]]]),np.diff([y[peak],y[peaks[i-1]]]))\n",
    "            if dist_next < avg_peak_dist or dist_prev < avg_peak_dist:\n",
    "                near_peaks.append(peak)\n",
    "    return near_peaks\n",
    "\n",
    "\n",
    "def find_peaks_(x,y):\n",
    "    \"\"\"finds the peak with scipy function\n",
    "    Saves the peaks of the original data that are an order higher than the noise.\n",
    "    For the noise peaks, saves only the peaks of a convolution between a Gaussian distrib. with 10 std and the \n",
    "    original function.\n",
    "    \"\"\"\n",
    "    peaks, _ = find_peaks(y)\n",
    "    \n",
    "    near_peaks = find_near_peaks(x,y,peaks)\n",
    "    \n",
    "    peaks_ = np.setdiff1d(peaks,near_peaks) # not near peaks\n",
    "\n",
    "    gaussian = signal.windows.gaussian(len(y), std=10)\n",
    "    \n",
    "    convolution = signal.fftconvolve(y, gaussian, mode='same')\n",
    "\n",
    "    peak_list, _ = find_peaks(convolution)\n",
    "\n",
    "    peaks_ = np.append(peaks_,peak_list)\n",
    "    \n",
    "    peaks_ = [p for p in peaks_ if x[p] <= 20000] #deletes the high frequency peaks (higher than 20kHz)\n",
    "    return peaks_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc747f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
